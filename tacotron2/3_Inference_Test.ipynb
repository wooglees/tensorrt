{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb22bb6",
   "metadata": {},
   "source": [
    "## Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat phrases/phrase.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710efa6e",
   "metadata": {},
   "source": [
    "# Tacotron2 + Waveglow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81034b0",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a88712",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf __pycache__\n",
    "!python inference.py -i phrases/phrase.txt --tacotron2 \"checkpoints/nvidia_tacotron2pyt_fp16.pt\" --waveglow \"checkpoints/nvidia_waveglow256pyt_fp16.pt\" --wn-channels 256 --sigma-infer 0.6 -o outputs/ --include-warmup --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "display(Audio(\"outputs/waveglow_audio_0.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b4ddb",
   "metadata": {},
   "source": [
    "### TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70284f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf __pycache__\n",
    "!python inference_trt.py -i phrases/phrase.txt --encoder outputs/encoder_fp16.engine --decoder outputs/decoder_iter_fp16.engine --postnet outputs/postnet_fp16.engine --waveglow outputs/waveglow_fp16.engine -o outputs/ --include-warmup --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b9202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "display(Audio(\"outputs/waveglow_audio_0_trt.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7122d21",
   "metadata": {},
   "source": [
    "### Latency Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee9eef4",
   "metadata": {},
   "source": [
    "Run ten times per each inference case, and then average the latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_latency_all = []\n",
    "trt_latency_all = []\n",
    "for i in range(10):\n",
    "    !rm -rf __pycache__\n",
    "    !python inference.py -i phrases/phrase.txt --tacotron2 \"checkpoints/nvidia_tacotron2pyt_fp16.pt\" --waveglow \"checkpoints/nvidia_waveglow256pyt_fp16.pt\" --wn-channels 256 --sigma-infer 0.6 -o outputs/ --include-warmup --fp16\n",
    "    latency = !cat logs/nvlog.json | tail -1 | awk '{print $(NF)}' | sed 's/}//g'\n",
    "    torch_latency_all = torch_latency_all + latency    \n",
    "    \n",
    "    !rm -rf __pycache__\n",
    "    !python inference_trt.py -i phrases/phrase.txt --encoder outputs/encoder_fp16.engine --decoder outputs/decoder_iter_fp16.engine --postnet outputs/postnet_fp16.engine --waveglow outputs/waveglow_fp16.engine -o outputs/ --include-warmup --fp16\n",
    "    latency = !cat logs/nvlog.json | tail -1 | awk '{print $(NF)}' | sed 's/}//g'\n",
    "    trt_latency_all = trt_latency_all + latency\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f16a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "torch_latency_all = [float(l) for l in torch_latency_all]\n",
    "torch_latency_mean = np.array(torch_latency_all).mean()\n",
    "torch_latency_std = np.array(torch_latency_all).std()\n",
    "print(\"PyTorch\")\n",
    "print(\"avg latency:\", torch_latency_mean)\n",
    "print(\"latency std:\", torch_latency_std)\n",
    "print(\"\")\n",
    "trt_latency_all = [float(l) for l in trt_latency_all]\n",
    "trt_latency_mean = np.array(trt_latency_all).mean()\n",
    "trt_latency_std = np.array(trt_latency_all).std()\n",
    "print(\"TensorRT\")\n",
    "print(\"avg latency:\", trt_latency_mean)\n",
    "print(\"latency std:\", trt_latency_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa6c7d",
   "metadata": {},
   "source": [
    "# Tacotron2 + ParallelWaveGan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f296db88",
   "metadata": {},
   "source": [
    "Some noises in the synthesis output due to the mel basis differnece between Tacotron2 and Parallelwavegan pretrained models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1414887",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf __pycache__\n",
    "!python inference.py -i phrases/phrase.txt --tacotron2 \"checkpoints/nvidia_tacotron2pyt_fp16.pt\" --parallelwavegan \"checkpoints/ljspeech_parallel_wavegan.v1.long/checkpoint-1000000steps.pkl\" -o outputs/ --include-warmup --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade4320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "display(Audio(\"outputs/parallelwavegan_audio_0.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339f46d",
   "metadata": {},
   "source": [
    "### TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe46267",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf __pycache__\n",
    "!python inference_trt.py -i phrases/phrase.txt --encoder outputs/encoder_fp16.engine --decoder outputs/decoder_iter_fp16.engine --postnet outputs/postnet_fp16.engine --parallelwavegan outputs/parallelwavegan_fp16.engine -o outputs/ --include-warmup --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "display(Audio(\"outputs/parallelwavegan_audio_0_trt.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4eceff",
   "metadata": {},
   "source": [
    "### Latency Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09742d77",
   "metadata": {},
   "source": [
    "Run ten times per each inference case, and average the latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwg_torch_latency_all = []\n",
    "pwg_trt_latency_all = []\n",
    "for i in range(10):\n",
    "    !rm -rf __pycache__\n",
    "    !python inference.py -i phrases/phrase.txt --tacotron2 \"checkpoints/nvidia_tacotron2pyt_fp16.pt\" --parallelwavegan \"checkpoints/ljspeech_parallel_wavegan.v1.long/checkpoint-1000000steps.pkl\" -o outputs/ --include-warmup --fp16\n",
    "    latency = !cat logs/nvlog.json | tail -1 | awk '{print $(NF)}' | sed 's/}//g'\n",
    "    pwg_torch_latency_all = pwg_torch_latency_all + latency    \n",
    "    \n",
    "    !rm -rf __pycache__\n",
    "    !python inference_trt.py -i phrases/phrase.txt --encoder outputs/encoder_fp16.engine --decoder outputs/decoder_iter_fp16.engine --postnet outputs/postnet_fp16.engine --parallelwavegan outputs/parallelwavegan_fp16.engine -o outputs/ --include-warmup --fp16\n",
    "    latency = !cat logs/nvlog.json | tail -1 | awk '{print $(NF)}' | sed 's/}//g'\n",
    "    pwg_trt_latency_all = pwg_trt_latency_all + latency\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ccf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pwg_torch_latency_all = [float(l) for l in pwg_torch_latency_all]\n",
    "pwg_torch_latency_mean = np.array(pwg_torch_latency_all).mean()\n",
    "pwg_torch_latency_std = np.array(pwg_torch_latency_all).std()\n",
    "print(\"PyTorch\")\n",
    "print(\"avg latency:\", pwg_torch_latency_mean)\n",
    "print(\"latency std:\", pwg_torch_latency_std)\n",
    "print(\"\")\n",
    "pwg_trt_latency_all = [float(l) for l in pwg_trt_latency_all]\n",
    "pwg_trt_latency_mean = np.array(pwg_trt_latency_all).mean()\n",
    "pwg_trt_latency_std = np.array(pwg_trt_latency_all).std()\n",
    "print(\"TensorRT\")\n",
    "print(\"avg latency:\", pwg_trt_latency_mean)\n",
    "print(\"latency std:\", pwg_trt_latency_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
