{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d657eac2",
   "metadata": {},
   "source": [
    "# Setup packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip3 install pycuda\n",
    "!pip3 install nvidia-dllogger\n",
    "!pip3 install nvidia-pyindex\n",
    "!pip3 install nvidia-tensorrt==8.4.3.1\n",
    "# !pip3 install torch-tensorrt -f https://github.com/NVIDIA/Torch-TensorRT/releases\n",
    "!pip3 install numpy scipy librosa unidecode inflect librosa colored\n",
    "!pip3 install onnxruntime onnx_graphsurgeon\n",
    "!pip3 install parallel_wavegan\n",
    "\n",
    "!apt-get update\n",
    "!apt-get install -y libsndfile1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80db5f",
   "metadata": {},
   "source": [
    "# Download models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab19698",
   "metadata": {},
   "source": [
    "### Tacotron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b9be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tacotron2 amp checkpoint from NGC\n",
    "!mkdir -p checkpoints\n",
    "!wget -nc --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/dle/tacotron2__pyt_ckpt/versions/19.12.0_amp/zip -O tacotron2__pyt_ckpt_19.12.0_amp.zip\n",
    "\n",
    "# Unzip and cp to ./checkpoints\n",
    "!unzip -o tacotron2__pyt_ckpt_19.12.0_amp.zip\n",
    "!mv nvidia_tacotron2pyt_fp16.pt ./checkpoints\n",
    "!rm tacotron2__pyt_ckpt_19.12.0_amp.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f73951",
   "metadata": {},
   "source": [
    "### Waveglow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16974164",
   "metadata": {},
   "source": [
    "You might need to setup **NGC** CLI in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49121537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download waveglow amp checkpoint\n",
    "!mkdir -p checkpoints\n",
    "# !ngc registry model download-version \"nvidia/waveglow256pyt_fp16:2\"  # or download the ckpt directly from https://ngc.nvidia.com/models/nvidia:waveglow256pyt_fp16\n",
    "\n",
    "# mv to ./checkpoints\n",
    "!mv waveglow256pyt_fp16_v2/waveglow_1076430_14000_amp checkpoints/nvidia_waveglow256pyt_fp16.pt\n",
    "!rmdir waveglow256pyt_fp16_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c307e98",
   "metadata": {},
   "source": [
    "### ParallelWaveGan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed761a",
   "metadata": {},
   "source": [
    "Referring https://github.com/kan-bayashi/ParallelWaveGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download parallelwavegan checkpoint\n",
    "from parallel_wavegan.utils import download_pretrained_model, PRETRAINED_MODEL_LIST\n",
    "print(PRETRAINED_MODEL_LIST.keys())\n",
    "download_pretrained_model(\"ljspeech_parallel_wavegan.v1.long\",\"checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d8de2b",
   "metadata": {},
   "source": [
    "# Simple inference test from the ckpts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71430b1a",
   "metadata": {},
   "source": [
    "### Tacotron2 + WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dfeb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple Inference FP16 test(Tacotron2 + Waveglow)\n",
    "# Define config parser\n",
    "class SoftDict:\n",
    "\tdef __init__(self, user_dict):\n",
    "\t\tself._user_dict = user_dict\n",
    "\t\tself._parse()\n",
    "\n",
    "\tdef _parse(self):\n",
    "\t\tfor key in self._user_dict.keys():\n",
    "\t\t\tvalue = self._user_dict[key]\n",
    "\t\t\tif type(value) == dict:\n",
    "\t\t\t\tvalue = SoftDict(value)\n",
    "\t\t\tsetattr(self, key, value)\n",
    "\n",
    "# Get Tacotron2 from the checkpoint\n",
    "import torch, models\n",
    "tacotron2_ckpt = torch.load(\"checkpoints/nvidia_tacotron2pyt_fp16.pt\")\n",
    "config = tacotron2_ckpt['config']\n",
    "config['fp16_run'] = True\n",
    "args = SoftDict(config)\n",
    "state_dict = tacotron2_ckpt['state_dict']\n",
    "for key in list(state_dict.keys()):\n",
    "    state_dict[key.replace(\"module.\",\"\")] = state_dict.pop(key)    \n",
    "    \n",
    "model_config = models.get_model_config(\"Tacotron2\", args)\n",
    "tacotron2 = models.get_model(\"Tacotron2\", model_config, False, forward_is_infer=True)\n",
    "tacotron2.load_state_dict(state_dict)\n",
    "tacotron2.eval()\n",
    "\n",
    "# Get Waveglow from the checkpoint\n",
    "# waveglow_ckpt = torch.load(\"checkpoints/waveglowpyt_fp16_20210323.pt\")\n",
    "waveglow_ckpt = torch.load(\"checkpoints/nvidia_waveglow256pyt_fp16.pt\")\n",
    "config = waveglow_ckpt['config']\n",
    "args = SoftDict(config)\n",
    "state_dict = waveglow_ckpt['state_dict']\n",
    "for key in list(state_dict.keys()):\n",
    "    state_dict[key.replace(\"module.\",\"\")] = state_dict.pop(key)  \n",
    "\n",
    "waveglow = models.get_model(\"WaveGlow\", config, False, forward_is_infer=True)\n",
    "waveglow.load_state_dict(state_dict)\n",
    "waveglow.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Sample text\n",
    "text = \"Hello my name is Woojin. Nice to meet you.\"\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')\n",
    "sequences, sequence_lengths = utils.prepare_input_sequence([text])\n",
    "\n",
    "# Infer\n",
    "with torch.no_grad():\n",
    "    rate = 22050\n",
    "    mel, mel_lengths, alignments = tacotron2(sequences, sequence_lengths)\n",
    "    audio = waveglow.infer(mel)\n",
    "    audio_numpy = audio[0].data.cpu().numpy()\n",
    "#     from scipy.io.wavfile import write\n",
    "#     write(\"audio.wav\", rate, audio_numpy)\n",
    "\n",
    "from IPython.display import Audio\n",
    "display(Audio(audio_numpy, rate=rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a90e77",
   "metadata": {},
   "source": [
    "### Tacotron2 + ParallelWaveGan\n",
    "\n",
    "Mel basis might be different between pretrained Tacotron2(from NGC) and ParallelWaveGan(from https://github.com/kan-bayashi/ParallelWaveGAN), which could causes lots of noise to the synthesis result. Just to check whether inference-flow works or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config parser\n",
    "class SoftDict:\n",
    "\tdef __init__(self, user_dict):\n",
    "\t\tself._user_dict = user_dict\n",
    "\t\tself._parse()\n",
    "\n",
    "\tdef _parse(self):\n",
    "\t\tfor key in self._user_dict.keys():\n",
    "\t\t\tvalue = self._user_dict[key]\n",
    "\t\t\tif type(value) == dict:\n",
    "\t\t\t\tvalue = SoftDict(value)\n",
    "\t\t\tsetattr(self, key, value)\n",
    "\n",
    "# Get Tacotron2 from the checkpoint\n",
    "import torch, models\n",
    "tacotron2_ckpt = torch.load(\"checkpoints/nvidia_tacotron2pyt_fp16.pt\")\n",
    "config = tacotron2_ckpt['config']\n",
    "config['fp16_run'] = True\n",
    "args = SoftDict(config)\n",
    "state_dict = tacotron2_ckpt['state_dict']\n",
    "for key in list(state_dict.keys()):\n",
    "    state_dict[key.replace(\"module.\",\"\")] = state_dict.pop(key)    \n",
    "    \n",
    "model_config = models.get_model_config(\"Tacotron2\", args)\n",
    "tacotron2 = models.get_model(\"Tacotron2\", model_config, False, forward_is_infer=True)\n",
    "tacotron2.load_state_dict(state_dict)\n",
    "tacotron2.eval()\n",
    "\n",
    "# Get ParallelWaveGan Generator\n",
    "import torch\n",
    "import yaml\n",
    "from parallel_wavegan.models import ParallelWaveGANGenerator\n",
    "\n",
    "pwg_ckpt=torch.load(\"checkpoints/ljspeech_parallel_wavegan.v1.long/checkpoint-1000000steps.pkl\")\n",
    "state_dict = pwg_ckpt['model']['generator']\n",
    "with open(\"checkpoints/ljspeech_parallel_wavegan.v1.long/config.yml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "generator_cfg = cfg['generator_params']\n",
    "\n",
    "parallelwavegan = ParallelWaveGANGenerator(**generator_cfg)\n",
    "parallelwavegan.load_state_dict(state_dict)\n",
    "parallelwavegan.half()\n",
    "parallelwavegan.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ee547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tacotron2_common.audio_processing import dynamic_range_decompression\n",
    "from parallel_wavegan.utils import read_hdf5\n",
    "\n",
    "# Setup Sample text\n",
    "text = \"Hello my name is Woojin. Nice to meet you.\"\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')\n",
    "sequences, sequence_lengths = utils.prepare_input_sequence([text])\n",
    "\n",
    "# Infer\n",
    "with torch.no_grad():\n",
    "    # Generate Spectrogram\n",
    "    mel, mel_lengths, alignments = tacotron2(sequences, sequence_lengths)\n",
    "    \n",
    "    # Decompress and log10 the output\n",
    "    decompressed = dynamic_range_decompression(mel)\n",
    "    decompressed_log10 = np.log10(decompressed.cpu()).cuda()\n",
    "    stats_path = \"checkpoints/ljspeech_parallel_wavegan.v1.long/stats.h5\"\n",
    "    mu = read_hdf5(stats_path, \"mean\")\n",
    "    sigma = read_hdf5(stats_path, \"scale\")\n",
    "    decompressed_log10_norm = (decompressed_log10 - torch.from_numpy(mu).view(1, -1, 1).cuda()) / torch.from_numpy(sigma).view(1, -1, 1).cuda()\n",
    "\n",
    "    # Prepare for inputs\n",
    "    rate = 22050\n",
    "    upsample_factor=256\n",
    "    c = decompressed_log10_norm.squeeze(0).transpose(0,1)\n",
    "    x = torch.randn(1, 1, len(c) * upsample_factor).to(\"cuda\")\n",
    "    c = c.transpose(1,0).unsqueeze(0)\n",
    "    c = torch.nn.ReplicationPad1d(2)(c)\n",
    "\n",
    "    x = x.half()\n",
    "    c = c.half()\n",
    "    \n",
    "    # Synthesis\n",
    "    pred = parallelwavegan(x,c).squeeze(0).transpose(1,0)\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "display(Audio(pred.view(-1).cpu().detach().numpy(), rate=rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
